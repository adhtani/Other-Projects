{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Sentiment Analysis - Extra\n",
    "\n",
    "#### DSC 550\n",
    "\n",
    "Taniya Adhikari 1/10/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\taniy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from re import sub\n",
    "import multiprocessing\n",
    "from unidecode import unidecode\n",
    "import unicodedata\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from time import time \n",
    "from collections import defaultdict\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "import textblob\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Pre-processing before modeling for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(df):\n",
    "    # converting all text to lowercase\n",
    "    df[\"comments\"] = df[\"comments\"].str.lower()\n",
    "    \n",
    "    # removing punctuation using string.punctuations and join()\n",
    "    df[\"comments\"] = df[\"comments\"].apply(lambda x: \"\".join([i for i in x if i not in string.punctuation]))\n",
    "    \n",
    "    # remove stop words\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    df['stopwords'] = df[\"comments\"].apply(lambda x: len([i for i in x.split() if i in stop]))\n",
    "    df[\"comments\"]= df[\"comments\"].apply(lambda x: \" \".join(i for i in x.split() if i not in stop))\n",
    "    df['stopwords'] = df['comments'].apply(lambda x: len([i for i in x.split() if i in stop]))\n",
    "    \n",
    "    # stemming\n",
    "    porter = PorterStemmer()\n",
    "    df[\"comments\"] = df[\"comments\"].apply(lambda x: \" \".join([porter.stem(word) for word in x.split()]))\n",
    "    return df\n",
    "\n",
    "# converting string into list of words for each comment\n",
    "def text_to_list(text):\n",
    "    text = unidecode(text)\n",
    "    pattern = r'[^A-Za-z ]'\n",
    "    regex = re.compile(pattern)\n",
    "    text = regex.sub('', text).split(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>comments</th>\n",
       "      <th>symbols</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14583</th>\n",
       "      <td>EnterpriseLeade</td>\n",
       "      <td>allianc data system ad research coverag start ...</td>\n",
       "      <td>ADS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19670</th>\n",
       "      <td>dakotafinancial</td>\n",
       "      <td>comerica cma upgrad outperform evercor isi htt...</td>\n",
       "      <td>CMA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18675</th>\n",
       "      <td>SeekingAlpha</td>\n",
       "      <td>amzn bbbi httpstco6oyrozzxzx</td>\n",
       "      <td>BBBY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20508</th>\n",
       "      <td>PotentTrading</td>\n",
       "      <td>top gapper higher volum eric nok ms mu</td>\n",
       "      <td>NOK</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12196</th>\n",
       "      <td>MarketBeatNews</td>\n",
       "      <td>san francisco consid tax compani help homeless...</td>\n",
       "      <td>MCK</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21923</th>\n",
       "      <td>Sumi57417058</td>\n",
       "      <td>rt 420invest üçÅ420investüçÅ fund marijuana opport...</td>\n",
       "      <td>ABBV</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16364</th>\n",
       "      <td>KarenMccaygirl</td>\n",
       "      <td>rt reuter ibm ask us juri award 167 million la...</td>\n",
       "      <td>GRPN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6725</th>\n",
       "      <td>ZolmaxNews</td>\n",
       "      <td>offic depot inc odp short interest updat https...</td>\n",
       "      <td>ODP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13600</th>\n",
       "      <td>dakotafinancial</td>\n",
       "      <td>pnc financi servic group inc forecast post fy2...</td>\n",
       "      <td>PNC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10617</th>\n",
       "      <td>mmahotstuff1</td>\n",
       "      <td>edison intern eix analyst see 091 ep httpstcow...</td>\n",
       "      <td>EIX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12800</th>\n",
       "      <td>TheMarketsDaily</td>\n",
       "      <td>nextera energi nee schedul post earn tuesday h...</td>\n",
       "      <td>NEE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12375</th>\n",
       "      <td>StocksThatGo</td>\n",
       "      <td>call day 716 mdg mtsl coda gene ped arc ambo a...</td>\n",
       "      <td>ARNC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23212</th>\n",
       "      <td>ledgerzette</td>\n",
       "      <td>marathon oil mro trade 67 httpstcodscuwhrbwd</td>\n",
       "      <td>MRO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22148</th>\n",
       "      <td>mastertraderMTA</td>\n",
       "      <td>rt mastertraderal htz booom short squeez stat ...</td>\n",
       "      <td>HTZ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27583</th>\n",
       "      <td>yon_yon_yon_yon</td>\n",
       "      <td>fake event aapl timestamp1531954801</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17119</th>\n",
       "      <td>rafatrade93</td>\n",
       "      <td>rt traderstewi momo went quiet mode us recent ...</td>\n",
       "      <td>MOMO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3244</th>\n",
       "      <td>whatsonthorold2</td>\n",
       "      <td>ep bmo shortterm us treasuri bond etf zt expec...</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10710</th>\n",
       "      <td>riyaisback</td>\n",
       "      <td>rt ajaydevfen kya khak maza hai jeen mein jab ...</td>\n",
       "      <td>IR</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14792</th>\n",
       "      <td>whatsonthorold2</td>\n",
       "      <td>automat data process inc adp‚Äô fuel run high re...</td>\n",
       "      <td>ADP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24219</th>\n",
       "      <td>KyawAung97</td>\n",
       "      <td>aaoi dip</td>\n",
       "      <td>AAOI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                source                                           comments  \\\n",
       "14583  EnterpriseLeade  allianc data system ad research coverag start ...   \n",
       "19670  dakotafinancial  comerica cma upgrad outperform evercor isi htt...   \n",
       "18675     SeekingAlpha                       amzn bbbi httpstco6oyrozzxzx   \n",
       "20508    PotentTrading             top gapper higher volum eric nok ms mu   \n",
       "12196   MarketBeatNews  san francisco consid tax compani help homeless...   \n",
       "21923     Sumi57417058  rt 420invest üçÅ420investüçÅ fund marijuana opport...   \n",
       "16364   KarenMccaygirl  rt reuter ibm ask us juri award 167 million la...   \n",
       "6725        ZolmaxNews  offic depot inc odp short interest updat https...   \n",
       "13600  dakotafinancial  pnc financi servic group inc forecast post fy2...   \n",
       "10617     mmahotstuff1  edison intern eix analyst see 091 ep httpstcow...   \n",
       "12800  TheMarketsDaily  nextera energi nee schedul post earn tuesday h...   \n",
       "12375     StocksThatGo  call day 716 mdg mtsl coda gene ped arc ambo a...   \n",
       "23212      ledgerzette       marathon oil mro trade 67 httpstcodscuwhrbwd   \n",
       "22148  mastertraderMTA  rt mastertraderal htz booom short squeez stat ...   \n",
       "27583  yon_yon_yon_yon                fake event aapl timestamp1531954801   \n",
       "17119      rafatrade93  rt traderstewi momo went quiet mode us recent ...   \n",
       "3244   whatsonthorold2  ep bmo shortterm us treasuri bond etf zt expec...   \n",
       "10710       riyaisback  rt ajaydevfen kya khak maza hai jeen mein jab ...   \n",
       "14792  whatsonthorold2  automat data process inc adp‚Äô fuel run high re...   \n",
       "24219       KyawAung97                                           aaoi dip   \n",
       "\n",
       "      symbols  stopwords  \n",
       "14583     ADS          0  \n",
       "19670     CMA          0  \n",
       "18675    BBBY          0  \n",
       "20508     NOK          0  \n",
       "12196     MCK          0  \n",
       "21923    ABBV          0  \n",
       "16364    GRPN          0  \n",
       "6725      ODP          0  \n",
       "13600     PNC          0  \n",
       "10617     EIX          0  \n",
       "12800     NEE          0  \n",
       "12375    ARNC          0  \n",
       "23212     MRO          0  \n",
       "22148     HTZ          0  \n",
       "27583    AAPL          0  \n",
       "17119    MOMO          0  \n",
       "3244      ZTS          0  \n",
       "10710      IR          0  \n",
       "14792     ADP          0  \n",
       "24219    AAOI          0  "
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('stockerbot-export.csv')\n",
    "df = df.dropna().drop_duplicates().reset_index(drop=True).rename(columns={'text':'comments'})\n",
    "df1 = df.sample(n = 20)\n",
    "df2 = df1.copy()\n",
    "df2 = pre_processing(df2)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source       20\n",
       "comments     20\n",
       "symbols      20\n",
       "stopwords    20\n",
       "dtype: int64"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = df2.copy()\n",
    "clean_df.comments = clean_df.comments.apply(lambda x: text_to_list(x))\n",
    "clean_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:05:30: collecting all words and their counts\n",
      "INFO - 14:05:30: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 14:05:30: PROGRESS: at sentence #7, processed 76 words and 139 word types\n",
      "INFO - 14:05:30: PROGRESS: at sentence #14, processed 144 words and 258 word types\n",
      "INFO - 14:05:30: collected 378 word types from a corpus of 214 words (unigram + bigrams) and 20 sentences\n",
      "INFO - 14:05:30: using 378 counts as vocab in Phrases<0 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 14:05:30: source_vocab length 378\n",
      "INFO - 14:05:30: Phraser built with 1 phrasegrams\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['aaoi', 'dip']"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extraction of phrases and bigram model\n",
    "sent = [row for row in clean_df.comments]\n",
    "phrases = Phrases(sent, min_count=1, progress_per=7)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]\n",
    "sentences[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 14:05:31: consider setting layer size to a multiple of 4 for greater performance\n",
      "INFO - 14:05:31: collecting all words and their counts\n",
      "INFO - 14:05:31: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 14:05:31: collected 183 word types from a corpus of 205 raw words and 20 sentences\n",
      "INFO - 14:05:31: Loading a fresh vocabulary\n",
      "INFO - 14:05:31: effective_min_count=1 retains 183 unique words (100% of original 183, drops 0)\n",
      "INFO - 14:05:31: effective_min_count=1 leaves 205 word corpus (100% of original 205, drops 0)\n",
      "INFO - 14:05:31: deleting the raw counts dictionary of 183 items\n",
      "INFO - 14:05:31: sample=0.001 downsamples 183 most-common words\n",
      "INFO - 14:05:31: downsampling leaves estimated 124 word corpus (60.6% of prior 205)\n",
      "INFO - 14:05:31: estimated required memory for 183 words and 2 dimensions: 94428 bytes\n",
      "INFO - 14:05:31: resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=1, size=2)\n",
    "\n",
    "# building vocab\n",
    "w2v_model.build_vocab(sentences, progress_per=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:05:31: training model with 3 workers on 183 vocabulary and 2 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO - 14:05:31: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:05:31: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:05:31: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:05:31: EPOCH - 1 : training on 205 raw words (126 effective words) took 0.0s, 36974 effective words/s\n",
      "INFO - 14:05:31: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:05:31: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:05:31: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:05:31: EPOCH - 2 : training on 205 raw words (139 effective words) took 0.0s, 58868 effective words/s\n",
      "INFO - 14:05:31: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:05:31: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:05:31: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:05:31: EPOCH - 3 : training on 205 raw words (134 effective words) took 0.0s, 49241 effective words/s\n",
      "INFO - 14:05:31: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 14:05:31: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 14:05:31: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 14:05:31: EPOCH - 4 : training on 205 raw words (131 effective words) took 0.0s, 36631 effective words/s\n",
      "INFO - 14:05:31: training on a 820 raw words (530 effective words) took 0.0s, 18542 effective words/s\n",
      "WARNING - 14:05:31: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "INFO - 14:05:31: precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=4, report_delay=1)\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=183, size=2, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:05:37: saving Word2Vec object under word2vec2.model, separately None\n",
      "INFO - 14:05:37: not storing attribute vectors_norm\n",
      "INFO - 14:05:37: not storing attribute cum_table\n",
      "INFO - 14:05:37: saved word2vec2.model\n"
     ]
    }
   ],
   "source": [
    "w2v_model.save(\"word2vec2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 14:05:38: loading Word2Vec object from word2vec2.model\n",
      "INFO - 14:05:38: loading wv recursively from word2vec2.model.wv.* with mmap=None\n",
      "INFO - 14:05:38: setting ignored attribute vectors_norm to None\n",
      "INFO - 14:05:38: loading vocabulary recursively from word2vec2.model.vocabulary.* with mmap=None\n",
      "INFO - 14:05:38: loading trainables recursively from word2vec2.model.trainables.* with mmap=None\n",
      "INFO - 14:05:38: setting ignored attribute cum_table to None\n",
      "INFO - 14:05:38: loaded word2vec2.model\n"
     ]
    }
   ],
   "source": [
    "word_vectors = Word2Vec.load(\"word2vec2.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means clustering to create sentiment dictionary for each word\n",
    "model = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=2).fit(X=word_vectors.vectors.astype(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ms', 0.999941885471344),\n",
       " ('marathon', 0.999916672706604),\n",
       " ('depot', 0.9993242621421814),\n",
       " ('fuel', 0.9989069104194641),\n",
       " ('squeez', 0.9967518448829651),\n",
       " ('gene', 0.9948847889900208),\n",
       " ('ko', 0.9946352243423462),\n",
       " ('award', 0.9944318532943726),\n",
       " ('mtsl', 0.9880948066711426),\n",
       " ('energi', 0.9870859384536743),\n",
       " ('mro', 0.9867139458656311),\n",
       " ('process', 0.9862383604049683),\n",
       " ('seen', 0.984703779220581),\n",
       " ('pnc', 0.9828438758850098),\n",
       " ('bmo', 0.9781846404075623),\n",
       " ('schedul', 0.9776513576507568),\n",
       " ('juri', 0.9754140973091125),\n",
       " ('timestamp', 0.9745939373970032),\n",
       " ('odp', 0.9716524481773376),\n",
       " ('gapper', 0.9616198539733887)]"
      ]
     },
     "execution_count": 700,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_vector(model.cluster_centers_[0], topn=20, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = model.cluster_centers_[0]\n",
    "negative = model.cluster_centers_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(word_vectors.vocab.keys())\n",
    "words.columns = ['words']\n",
    "words['vectors'] = words.words.apply(lambda x: word_vectors[f'{x}'])\n",
    "words['cluster'] = words.vectors.apply(lambda x: model.predict([np.array(x)]))\n",
    "words.cluster = words.cluster.apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_value</th>\n",
       "      <th>closeness_score</th>\n",
       "      <th>sentiment_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allianc</td>\n",
       "      <td>[0.6164918, -0.7873613]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.886811</td>\n",
       "      <td>0.886811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data</td>\n",
       "      <td>[0.68229717, -0.7310749]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.931173</td>\n",
       "      <td>0.931173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>system</td>\n",
       "      <td>[-0.4534326, -0.89129055]</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.416431</td>\n",
       "      <td>-1.416431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ad</td>\n",
       "      <td>[-0.90975183, 0.41515255]</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.467843</td>\n",
       "      <td>-1.467843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>research</td>\n",
       "      <td>[0.98846287, 0.15146352]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.352660</td>\n",
       "      <td>2.352660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>less</td>\n",
       "      <td>[-0.6440829, 0.76495564]</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.025363</td>\n",
       "      <td>-1.025363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>seller</td>\n",
       "      <td>[0.65319747, 0.75718755]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.384190</td>\n",
       "      <td>2.384190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>httpstcoedeswcykz</td>\n",
       "      <td>[0.5230623, 0.8522945]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.926815</td>\n",
       "      <td>1.926815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>aaoi</td>\n",
       "      <td>[-0.87779075, -0.47904414]</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2.785060</td>\n",
       "      <td>-2.785060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>dip</td>\n",
       "      <td>[0.7856866, -0.6186247]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.025371</td>\n",
       "      <td>1.025371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 words                     vectors  cluster  cluster_value  \\\n",
       "0              allianc     [0.6164918, -0.7873613]        0              1   \n",
       "1                 data    [0.68229717, -0.7310749]        0              1   \n",
       "2               system   [-0.4534326, -0.89129055]        1             -1   \n",
       "3                   ad   [-0.90975183, 0.41515255]        1             -1   \n",
       "4             research    [0.98846287, 0.15146352]        0              1   \n",
       "..                 ...                         ...      ...            ...   \n",
       "178               less    [-0.6440829, 0.76495564]        1             -1   \n",
       "179             seller    [0.65319747, 0.75718755]        0              1   \n",
       "180  httpstcoedeswcykz      [0.5230623, 0.8522945]        0              1   \n",
       "181               aaoi  [-0.87779075, -0.47904414]        1             -1   \n",
       "182                dip     [0.7856866, -0.6186247]        0              1   \n",
       "\n",
       "     closeness_score  sentiment_coeff  \n",
       "0           0.886811         0.886811  \n",
       "1           0.931173         0.931173  \n",
       "2           1.416431        -1.416431  \n",
       "3           1.467843        -1.467843  \n",
       "4           2.352660         2.352660  \n",
       "..               ...              ...  \n",
       "178         1.025363        -1.025363  \n",
       "179         2.384190         2.384190  \n",
       "180         1.926815         1.926815  \n",
       "181         2.785060        -2.785060  \n",
       "182         1.025371         1.025371  \n",
       "\n",
       "[183 rows x 6 columns]"
      ]
     },
     "execution_count": 703,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words['cluster_value'] = [1 if i==0 else -1 for i in words.cluster]\n",
    "words['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)\n",
    "words['sentiment_coeff'] = words.closeness_score * words.cluster_value\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer(wordlist):\n",
    "    cluster_count = {}\n",
    "    for w in wordlist:\n",
    "        x = cluster_freq(w, words)\n",
    "        if x in cluster_count:\n",
    "            cluster_count[x] = cluster_count[x] + 1\n",
    "        else:\n",
    "            cluster_count[x] = 1\n",
    "    sentiment = \"\"              \n",
    "    if '-1' in cluster_count.keys() and '1' in cluster_count.keys():\n",
    "        pos = cluster_count.get('1')\n",
    "        neg = cluster_count.get('-1')\n",
    "        if pos >= neg:\n",
    "            sentiment = \"Positive\"\n",
    "        else:\n",
    "            sentiment = \"Negative\"\n",
    "\n",
    "    elif '-1' not in cluster_count.keys():\n",
    "        sentiment = \"Positive\"\n",
    "    elif '1' not in cluster_count.keys():\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        None\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_freq(word, df):\n",
    "    x = ''\n",
    "    for i, r in df.iterrows():\n",
    "        if r['words'] == word:\n",
    "            x = str(r['cluster_value'])\n",
    "        else:\n",
    "            None\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_count = {}\n",
    "sentiment = []\n",
    "result = \"\"\n",
    "for i, r in df2.iterrows():\n",
    "    comment = r['comments']\n",
    "    wordlist = comment.split()\n",
    "    result = sentiment_analyzer(wordlist)\n",
    "    sentiment.append(result)\n",
    "    \n",
    "df1['sentiment'] = sentiment    \n",
    "df2['sentiment'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_func(comments):\n",
    "    pol = TextBlob(comments).sentiment.polarity\n",
    "    if pol >= 0:\n",
    "        x = \"Positive\"\n",
    "    else:\n",
    "        x = \"Negative\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>comments</th>\n",
       "      <th>symbols</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>textblob_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14583</th>\n",
       "      <td>EnterpriseLeade</td>\n",
       "      <td>Alliance Data Systems $ADS Research Coverage S...</td>\n",
       "      <td>ADS</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19670</th>\n",
       "      <td>dakotafinancial</td>\n",
       "      <td>Comerica $CMA Upgraded to Outperform at Everco...</td>\n",
       "      <td>CMA</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18675</th>\n",
       "      <td>SeekingAlpha</td>\n",
       "      <td>$AMZN $BBBY https://t.co/6oyROZZxZx</td>\n",
       "      <td>BBBY</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20508</th>\n",
       "      <td>PotentTrading</td>\n",
       "      <td>Top gappers (up) with higher volume: $ERIC $NO...</td>\n",
       "      <td>NOK</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12196</th>\n",
       "      <td>MarketBeatNews</td>\n",
       "      <td>San Francisco to consider tax on companies to ...</td>\n",
       "      <td>MCK</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21923</th>\n",
       "      <td>Sumi57417058</td>\n",
       "      <td>RT @420_invest: üçÅ@420_investüçÅ  We fund #mariju...</td>\n",
       "      <td>ABBV</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16364</th>\n",
       "      <td>KarenMccaygirl</td>\n",
       "      <td>RT @Reuters: IBM asks a U.S. jury to award it ...</td>\n",
       "      <td>GRPN</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6725</th>\n",
       "      <td>ZolmaxNews</td>\n",
       "      <td>Office Depot Inc $ODP Short Interest Update ht...</td>\n",
       "      <td>ODP</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13600</th>\n",
       "      <td>dakotafinancial</td>\n",
       "      <td>PNC Financial Services Group Inc Forecasted to...</td>\n",
       "      <td>PNC</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10617</th>\n",
       "      <td>mmahotstuff1</td>\n",
       "      <td>Edison International $EIX Analysts See $0.91 E...</td>\n",
       "      <td>EIX</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12800</th>\n",
       "      <td>TheMarketsDaily</td>\n",
       "      <td>NextEra Energy $NEE Scheduled to Post Earnings...</td>\n",
       "      <td>NEE</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12375</th>\n",
       "      <td>StocksThatGo</td>\n",
       "      <td>Call of the day 7/16 $MDGS $MTSL $CODA $GENE $...</td>\n",
       "      <td>ARNC</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23212</th>\n",
       "      <td>ledgerzette</td>\n",
       "      <td>Marathon Oil $MRO Trading Down 6.7% https://t....</td>\n",
       "      <td>MRO</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22148</th>\n",
       "      <td>mastertraderMTA</td>\n",
       "      <td>RT @MasterTraderAle: $HTZ booom. Short squeeze...</td>\n",
       "      <td>HTZ</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27583</th>\n",
       "      <td>yon_yon_yon_yon</td>\n",
       "      <td>Fake event for $AAPL timestamp:1531954801</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17119</th>\n",
       "      <td>rafatrade93</td>\n",
       "      <td>RT @traderstewie: $MOMO .... went into \"quiet\"...</td>\n",
       "      <td>MOMO</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3244</th>\n",
       "      <td>whatsonthorold2</td>\n",
       "      <td>EPS for BMO Short-Term US Treasury Bond ETF $Z...</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10710</th>\n",
       "      <td>riyaisback</td>\n",
       "      <td>RT @ajaydevfen: Kya khak #Maza Hai Jeene Mein ...</td>\n",
       "      <td>IR</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14792</th>\n",
       "      <td>whatsonthorold2</td>\n",
       "      <td>Is Automatic Data Processing Inc $ADP‚Äôs Fuel R...</td>\n",
       "      <td>ADP</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24219</th>\n",
       "      <td>KyawAung97</td>\n",
       "      <td>$AAOI what a dip</td>\n",
       "      <td>AAOI</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                source                                           comments  \\\n",
       "14583  EnterpriseLeade  Alliance Data Systems $ADS Research Coverage S...   \n",
       "19670  dakotafinancial  Comerica $CMA Upgraded to Outperform at Everco...   \n",
       "18675     SeekingAlpha                $AMZN $BBBY https://t.co/6oyROZZxZx   \n",
       "20508    PotentTrading  Top gappers (up) with higher volume: $ERIC $NO...   \n",
       "12196   MarketBeatNews  San Francisco to consider tax on companies to ...   \n",
       "21923     Sumi57417058  RT @420_invest: üçÅ@420_investüçÅ  We fund #mariju...   \n",
       "16364   KarenMccaygirl  RT @Reuters: IBM asks a U.S. jury to award it ...   \n",
       "6725        ZolmaxNews  Office Depot Inc $ODP Short Interest Update ht...   \n",
       "13600  dakotafinancial  PNC Financial Services Group Inc Forecasted to...   \n",
       "10617     mmahotstuff1  Edison International $EIX Analysts See $0.91 E...   \n",
       "12800  TheMarketsDaily  NextEra Energy $NEE Scheduled to Post Earnings...   \n",
       "12375     StocksThatGo  Call of the day 7/16 $MDGS $MTSL $CODA $GENE $...   \n",
       "23212      ledgerzette  Marathon Oil $MRO Trading Down 6.7% https://t....   \n",
       "22148  mastertraderMTA  RT @MasterTraderAle: $HTZ booom. Short squeeze...   \n",
       "27583  yon_yon_yon_yon          Fake event for $AAPL timestamp:1531954801   \n",
       "17119      rafatrade93  RT @traderstewie: $MOMO .... went into \"quiet\"...   \n",
       "3244   whatsonthorold2  EPS for BMO Short-Term US Treasury Bond ETF $Z...   \n",
       "10710       riyaisback  RT @ajaydevfen: Kya khak #Maza Hai Jeene Mein ...   \n",
       "14792  whatsonthorold2  Is Automatic Data Processing Inc $ADP‚Äôs Fuel R...   \n",
       "24219       KyawAung97                                   $AAOI what a dip   \n",
       "\n",
       "      symbols sentiment textblob_sentiment  \n",
       "14583     ADS  Positive           Positive  \n",
       "19670     CMA  Negative           Positive  \n",
       "18675    BBBY  Positive           Positive  \n",
       "20508     NOK  Positive           Positive  \n",
       "12196     MCK  Positive           Positive  \n",
       "21923    ABBV  Negative           Positive  \n",
       "16364    GRPN  Negative           Positive  \n",
       "6725      ODP  Negative           Positive  \n",
       "13600     PNC  Positive           Positive  \n",
       "10617     EIX  Negative           Positive  \n",
       "12800     NEE  Positive           Positive  \n",
       "12375    ARNC  Negative           Positive  \n",
       "23212     MRO  Positive           Positive  \n",
       "22148     HTZ  Positive           Positive  \n",
       "27583    AAPL  Positive           Negative  \n",
       "17119    MOMO  Negative           Positive  \n",
       "3244      ZTS  Negative           Positive  \n",
       "10710      IR  Positive           Positive  \n",
       "14792     ADP  Positive           Negative  \n",
       "24219    AAOI  Positive           Positive  "
      ]
     },
     "execution_count": 708,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['textblob_sentiment']= df2['comments'].apply(sentiment_func)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm I created to create a sentiment for comment does not work with bigger dataset. However, up until kmeans, the code works fine. For sentiment prediction it will need a different method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
